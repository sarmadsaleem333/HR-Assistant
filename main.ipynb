{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4901dcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting PyPDF2\n",
      "  Using cached pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting python-docx\n",
      "  Downloading python_docx-1.2.0-py3-none-any.whl.metadata (2.0 kB)\n",
      "Collecting pytesseract\n",
      "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting pdf2image\n",
      "  Downloading pdf2image-1.17.0-py3-none-any.whl.metadata (6.2 kB)\n",
      "Collecting Pillow\n",
      "  Downloading pillow-11.3.0-cp39-cp39-win_amd64.whl.metadata (9.2 kB)\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "     ---------------------------------------- 0.0/981.5 kB ? eta -:--:--\n",
      "     -------------------- ----------------- 524.3/981.5 kB 5.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 981.5/981.5 kB 2.9 MB/s  0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: typing_extensions>=3.10.0.0 in d:\\anaconda\\envs\\hr-assistant\\lib\\site-packages (from PyPDF2) (4.14.1)\n",
      "Collecting lxml>=3.1.0 (from python-docx)\n",
      "  Downloading lxml-6.0.2-cp39-cp39-win_amd64.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: packaging>=21.3 in d:\\anaconda\\envs\\hr-assistant\\lib\\site-packages (from pytesseract) (25.0)\n",
      "Requirement already satisfied: six in d:\\anaconda\\envs\\hr-assistant\\lib\\site-packages (from langdetect) (1.17.0)\n",
      "Using cached pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
      "Downloading python_docx-1.2.0-py3-none-any.whl (252 kB)\n",
      "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
      "Downloading pdf2image-1.17.0-py3-none-any.whl (11 kB)\n",
      "Downloading pillow-11.3.0-cp39-cp39-win_amd64.whl (7.0 MB)\n",
      "   ---------------------------------------- 0.0/7.0 MB ? eta -:--:--\n",
      "   ------- -------------------------------- 1.3/7.0 MB 6.1 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 2.9/7.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 4.5/7.0 MB 7.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 6.0/7.0 MB 7.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  6.8/7.0 MB 7.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 7.0/7.0 MB 6.6 MB/s  0:00:01\n",
      "Downloading lxml-6.0.2-cp39-cp39-win_amd64.whl (4.0 MB)\n",
      "   ---------------------------------------- 0.0/4.0 MB ? eta -:--:--\n",
      "   ------------ --------------------------- 1.3/4.0 MB 6.7 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 2.6/4.0 MB 6.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 3.7/4.0 MB 7.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 4.0/4.0 MB 5.8 MB/s  0:00:00\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml): started\n",
      "  Building wheel for langdetect (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993363 sha256=ac383327995bcaa4e1ef8a63fd3583eebd04ce039e078ae0c8f5cae4d8a56b6d\n",
      "  Stored in directory: c:\\users\\sarma\\appdata\\local\\pip\\cache\\wheels\\d1\\c1\\d9\\7e068de779d863bc8f8fc9467d85e25cfe47fa5051fff1a1bb\n",
      "Successfully built langdetect\n",
      "Installing collected packages: PyPDF2, Pillow, lxml, langdetect, python-docx, pytesseract, pdf2image\n",
      "\n",
      "   ---------------------------------------- 0/7 [PyPDF2]\n",
      "   ---------------------------------------- 0/7 [PyPDF2]\n",
      "   ---------------------------------------- 0/7 [PyPDF2]\n",
      "   ----- ---------------------------------- 1/7 [Pillow]\n",
      "   ----- ---------------------------------- 1/7 [Pillow]\n",
      "   ----- ---------------------------------- 1/7 [Pillow]\n",
      "   ----- ---------------------------------- 1/7 [Pillow]\n",
      "   ----- ---------------------------------- 1/7 [Pillow]\n",
      "   ----- ---------------------------------- 1/7 [Pillow]\n",
      "   ----- ---------------------------------- 1/7 [Pillow]\n",
      "   ----- ---------------------------------- 1/7 [Pillow]\n",
      "   ----- ---------------------------------- 1/7 [Pillow]\n",
      "   ----------- ---------------------------- 2/7 [lxml]\n",
      "   ----------- ---------------------------- 2/7 [lxml]\n",
      "   ----------- ---------------------------- 2/7 [lxml]\n",
      "   ----------- ---------------------------- 2/7 [lxml]\n",
      "   ----------- ---------------------------- 2/7 [lxml]\n",
      "   ----------- ---------------------------- 2/7 [lxml]\n",
      "   ----------------- ---------------------- 3/7 [langdetect]\n",
      "   ----------------- ---------------------- 3/7 [langdetect]\n",
      "   ----------------- ---------------------- 3/7 [langdetect]\n",
      "   ----------------- ---------------------- 3/7 [langdetect]\n",
      "   ----------------- ---------------------- 3/7 [langdetect]\n",
      "   ---------------------- ----------------- 4/7 [python-docx]\n",
      "   ---------------------- ----------------- 4/7 [python-docx]\n",
      "   ---------------------- ----------------- 4/7 [python-docx]\n",
      "   ---------------------- ----------------- 4/7 [python-docx]\n",
      "   ---------------------- ----------------- 4/7 [python-docx]\n",
      "   ---------------------- ----------------- 4/7 [python-docx]\n",
      "   ---------------------- ----------------- 4/7 [python-docx]\n",
      "   ---------------------- ----------------- 4/7 [python-docx]\n",
      "   ---------------------------- ----------- 5/7 [pytesseract]\n",
      "   ---------------------------------------- 7/7 [pdf2image]\n",
      "\n",
      "Successfully installed Pillow-11.3.0 PyPDF2-3.0.1 langdetect-1.0.9 lxml-6.0.2 pdf2image-1.17.0 pytesseract-0.3.13 python-docx-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install PyPDF2 python-docx pytesseract pdf2image Pillow langdetect google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dccb0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning done. Log saved to docs/cleaning_log.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from langdetect import detect\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "import re\n",
    "from PIL import Image\n",
    "# Folders\n",
    "RAW_DIR = \"data/raw\"\n",
    "CLEAN_DIR = \"data/cleaned\"\n",
    "LOG_FILE = \"docs/cleaning_log.txt\"\n",
    "\n",
    "Path(CLEAN_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(\"docs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to redact emails and phone numbers\n",
    "def redact_pii(text):\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '[REDACTED_EMAIL]', text)\n",
    "    text = re.sub(r'\\b\\d{10,}\\b', '[REDACTED_PHONE]', text)\n",
    "    return text\n",
    "\n",
    "def is_pdf(file_path):\n",
    "    return file_path.lower().endswith(\".pdf\")\n",
    "\n",
    "def is_docx(file_path):\n",
    "    return file_path.lower().endswith(\".docx\")\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def extract_text_pdf(file_path):\n",
    "    try:\n",
    "        reader = PdfReader(file_path)\n",
    "        text = \"\"\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text() or \"\"\n",
    "        # If no text found → scanned PDF → use OCR\n",
    "        if len(text.strip()) == 0:\n",
    "            images = convert_from_path(file_path)\n",
    "            text = \"\"\n",
    "            for img in images:\n",
    "                text += pytesseract.image_to_string(img)\n",
    "        return text\n",
    "    except:\n",
    "        return None\n",
    "   \n",
    "# import fitz  # PyMuPDF\n",
    "# import pytesseract\n",
    "\n",
    "# def extract_text_pdf(file_path):\n",
    "#     try:\n",
    "#         doc = fitz.open(file_path)\n",
    "#         text = \"\"\n",
    "#         for page in doc:\n",
    "#             # Extract text directly\n",
    "#             page_text = page.get_text()\n",
    "#             if page_text.strip():\n",
    "#                 text += page_text\n",
    "#             else:\n",
    "#                 # If no text, use OCR on the page image\n",
    "#                 pix = page.get_pixmap(dpi=300)\n",
    "#                 img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "#                 text += pytesseract.image_to_string(img, lang='eng')\n",
    "#         return text if text.strip() else None\n",
    "#     except Exception as e:\n",
    "#         print(f\"Error reading PDF {file_path}: {e}\")\n",
    "#         return None\n",
    "\n",
    "def extract_text_docx(file_path):\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_files():\n",
    "    cleaning_log = []\n",
    "    seen_files = set()\n",
    "    for filename in os.listdir(RAW_DIR):\n",
    "        file_path = os.path.join(RAW_DIR, filename)\n",
    "\n",
    "        # Skip duplicates\n",
    "        if filename in seen_files:\n",
    "            cleaning_log.append((filename, \"duplicate\"))\n",
    "            continue\n",
    "        seen_files.add(filename)\n",
    "\n",
    "        # Unsupported format\n",
    "        if not (is_pdf(file_path) or is_docx(file_path)):\n",
    "            cleaning_log.append((filename, \"unsupported format\"))\n",
    "            continue\n",
    "\n",
    "        # Extract text\n",
    "        text = extract_text_pdf(file_path) if is_pdf(file_path) else extract_text_docx(file_path)\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            cleaning_log.append((filename, \"corrupted or empty\"))\n",
    "            continue\n",
    "\n",
    "        # Detect language\n",
    "        lang = detect_language(text)\n",
    "        if lang != \"en\":\n",
    "            cleaning_log.append((filename, f\"non-English ({lang})\"))\n",
    "            continue\n",
    "\n",
    "        # Redact PII\n",
    "        text = redact_pii(text)\n",
    "\n",
    "        # Save cleaned file\n",
    "        shutil.copy(file_path, os.path.join(CLEAN_DIR, filename))\n",
    "        cleaning_log.append((filename, \"cleaned\"))\n",
    "\n",
    "    # Save log\n",
    "    with open(LOG_FILE, \"w\") as f:\n",
    "        for item in cleaning_log:\n",
    "            f.write(f\"{item[0]} : {item[1]}\\n\")\n",
    "    print(f\"Cleaning done. Log saved to {LOG_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3fdf7e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing done. Text files saved in data/text\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from PyPDF2 import PdfReader\n",
    "from docx import Document\n",
    "\n",
    "# Folders\n",
    "CLEAN_DIR = \"data/cleaned\"\n",
    "TEXT_DIR = \"data/text\"\n",
    "\n",
    "# Create text folder if it doesn't exist\n",
    "Path(TEXT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Extract text from PDF\n",
    "def extract_pdf(file_path):\n",
    "    reader = PdfReader(file_path)\n",
    "    text = \"\"\n",
    "    for page in reader.pages:\n",
    "        text += page.extract_text() or \"\"\n",
    "    return text\n",
    "\n",
    "# Extract text from DOCX\n",
    "def extract_docx(file_path):\n",
    "    doc = Document(file_path)\n",
    "    return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "\n",
    "# Parse all cleaned CVs\n",
    "for filename in os.listdir(CLEAN_DIR):\n",
    "    file_path = os.path.join(CLEAN_DIR, filename)\n",
    "    text = \"\"\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        text = extract_pdf(file_path)\n",
    "    elif filename.lower().endswith(\".docx\"):\n",
    "        text = extract_docx(file_path)\n",
    "    else:\n",
    "        continue  # skip unsupported files\n",
    "\n",
    "    # Save extracted text\n",
    "    text_file = os.path.join(TEXT_DIR, filename + \".txt\")\n",
    "    with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "print(f\"Parsing done. Text files saved in {TEXT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ce9dc3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON extraction done. Files saved in data/json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "TEXT_DIR = \"data/text\"\n",
    "JSON_DIR = \"data/json\"\n",
    "\n",
    "Path(JSON_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_education(text):\n",
    "    education = []\n",
    "    # Simple regex examples\n",
    "    edu_matches = re.findall(r\"(BSc|MSc|PhD|Bachelor|Master|Doctor).+?([0-9]{4})\\s*-\\s*([0-9]{4})\", text)\n",
    "    for match in edu_matches:\n",
    "        degree, start, end = match\n",
    "        education.append({\n",
    "            \"degree\": degree,\n",
    "            \"field\": \"\",  # could be parsed further\n",
    "            \"university\": \"\",\n",
    "            \"country\": \"\",\n",
    "            \"start\": int(start),\n",
    "            \"end\": int(end),\n",
    "            \"gpa\": None,\n",
    "            \"scale\": None\n",
    "        })\n",
    "    return education\n",
    "\n",
    "def extract_experience(text):\n",
    "    experience = []\n",
    "    exp_matches = re.findall(r\"(\\w.+?),\\s*(\\w.+?),\\s*([0-9]{4}).*?([0-9]{4})\", text)\n",
    "    for match in exp_matches:\n",
    "        title, org, start, end = match\n",
    "        experience.append({\n",
    "            \"title\": title,\n",
    "            \"org\": org,\n",
    "            \"start\": int(start),\n",
    "            \"end\": int(end),\n",
    "            \"duration_months\": (int(end)-int(start))*12,\n",
    "            \"domain\": \"\"\n",
    "        })\n",
    "    return experience\n",
    "\n",
    "def extract_publications(text):\n",
    "    publications = []\n",
    "    pub_matches = re.findall(r'\"(.+?)\",\\s*(.+?),\\s*([0-9]{4})', text)\n",
    "    for match in pub_matches:\n",
    "        title, venue, year = match\n",
    "        publications.append({\n",
    "            \"title\": title,\n",
    "            \"venue\": venue,\n",
    "            \"year\": int(year),\n",
    "            \"type\": \"conference\",\n",
    "            \"authors\": [],\n",
    "            \"author_position\": None,\n",
    "            \"journal_if\": None,\n",
    "            \"domain\": \"\"\n",
    "        })\n",
    "    return publications\n",
    "\n",
    "def extract_awards(text):\n",
    "    awards = []\n",
    "    award_matches = re.findall(r'Best Paper Award|Awarded', text)\n",
    "    for match in award_matches:\n",
    "        awards.append({\n",
    "            \"title\": match,\n",
    "            \"issuer\": \"\",\n",
    "            \"year\": None,\n",
    "            \"type\": \"award\"\n",
    "        })\n",
    "    return awards\n",
    "\n",
    "for filename in os.listdir(TEXT_DIR):\n",
    "    file_path = os.path.join(TEXT_DIR, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    data = {\n",
    "        \"education\": extract_education(text),\n",
    "        \"experience\": extract_experience(text),\n",
    "        \"publications\": extract_publications(text),\n",
    "        \"awards\": extract_awards(text)\n",
    "    }\n",
    "\n",
    "    json_file = os.path.join(JSON_DIR, filename.replace(\".txt\", \".json\"))\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "print(f\"JSON extraction done. Files saved in {JSON_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4708a422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-dotenv\n",
      "  Downloading python_dotenv-1.2.1-py3-none-any.whl.metadata (25 kB)\n",
      "Downloading python_dotenv-1.2.1-py3-none-any.whl (21 kB)\n",
      "Installing collected packages: python-dotenv\n",
      "Successfully installed python-dotenv-1.2.1\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fff66840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini JSON extraction done. Raw responses in data/raw, cleaned JSON in data/json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "# ---------------------- CONFIG ----------------------\n",
    "TEXT_DIR = \"data/text\"\n",
    "JSON_DIR = \"data/json\"\n",
    "RAW_DIR = \"data/raw\"  # Store raw LLM responses\n",
    "Path(JSON_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(RAW_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# ---------------------- FUNCTION ----------------------\n",
    "def extract_json_with_gemini(text):\n",
    "    \"\"\"\n",
    "    Send CV text to Gemini LLM and get structured JSON safely.\n",
    "    Returns (parsed_json, raw_response_text)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Extract structured CV information in the following JSON schema:\n",
    "    {{\n",
    "      \"education\": [{{\"degree\":\"\",\"field\":\"\",\"university\":\"\",\"country\":\"\",\"start\":null,\"end\":null,\"gpa\":null,\"scale\":null}}],\n",
    "      \"experience\": [{{\"title\":\"\",\"org\":\"\",\"start\":null,\"end\":null,\"duration_months\":null,\"domain\":\"\"}}],\n",
    "      \"publications\": [{{\"title\":\"\",\"venue\":\"\",\"year\":null,\"type\":\"\",\"authors\":[],\"author_position\":null,\"journal_if\":null,\"domain\":\"\"}}],\n",
    "      \"awards\": [{{\"title\":\"\",\"issuer\":\"\",\"year\":null,\"type\":\"\"}}]\n",
    "    }}\n",
    "\n",
    "    CV Text:\n",
    "    {text}\n",
    "\n",
    "    Return **only valid JSON**. Do not add any explanation or extra text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call Gemini model\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0)\n",
    "    )\n",
    "\n",
    "    raw_text = resp.text.strip()\n",
    "\n",
    "    # Save raw response for auditing\n",
    "    raw_text_cleaned = raw_text.replace(\"\\ufeff\", \"\")  # remove BOM if any\n",
    "\n",
    "    # Attempt to parse JSON safely\n",
    "    parsed_json = None\n",
    "    if raw_text_cleaned:\n",
    "        try:\n",
    "            parsed_json = json.loads(raw_text_cleaned)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fix common issues: trailing commas, extra text\n",
    "            first_brace = raw_text_cleaned.find(\"{\")\n",
    "            last_brace = raw_text_cleaned.rfind(\"}\")\n",
    "            if first_brace != -1 and last_brace != -1:\n",
    "                raw_text_cleaned = raw_text_cleaned[first_brace:last_brace+1]\n",
    "            # remove trailing commas\n",
    "            raw_text_cleaned = re.sub(r',\\s*([}\\]])', r'\\1', raw_text_cleaned)\n",
    "            try:\n",
    "                parsed_json = json.loads(raw_text_cleaned)\n",
    "            except json.JSONDecodeError:\n",
    "                # Fallback to empty schema\n",
    "                parsed_json = {\"education\": [], \"experience\": [], \"publications\": [], \"awards\": []}\n",
    "\n",
    "    else:\n",
    "        parsed_json = {\"education\": [], \"experience\": [], \"publications\": [], \"awards\": []}\n",
    "\n",
    "    return parsed_json, raw_text\n",
    "\n",
    "# ---------------------- MAIN LOOP ----------------------\n",
    "for filename in os.listdir(TEXT_DIR):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "    file_path = os.path.join(TEXT_DIR, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Extract JSON using Gemini\n",
    "    structured_json, raw_resp = extract_json_with_gemini(text)\n",
    "\n",
    "    # Save raw response\n",
    "    raw_file = os.path.join(RAW_DIR, filename.replace(\".txt\", \"_raw.txt\"))\n",
    "    with open(raw_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_resp)\n",
    "\n",
    "    # Save cleaned JSON\n",
    "    json_file = os.path.join(JSON_DIR, filename.replace(\".txt\", \".json\"))\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(structured_json, f, indent=2)\n",
    "\n",
    "print(f\"Gemini JSON extraction done. Raw responses in {RAW_DIR}, cleaned JSON in {JSON_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc43be",
   "metadata": {},
   "source": [
    "With regex sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745078d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sections(text):\n",
    "    \"\"\"\n",
    "    Extract main CV sections using regex.\n",
    "    Returns a dictionary with keys: 'education', 'experience', 'publications', 'awards'\n",
    "    \"\"\"\n",
    "    sections = {'education': '', 'experience': '', 'publications': '', 'awards': ''}\n",
    "    \n",
    "    # Define regex patterns for section headers (case-insensitive)\n",
    "    patterns = {\n",
    "        'education': r\"(Education|Academic Background|Academic Qualifications)(.*?)(?=\\n[A-Z][a-zA-Z ]{2,}:|\\Z)\",\n",
    "        'experience': r\"(Experience|Work History|Employment)(.*?)(?=\\n[A-Z][a-zA-Z ]{2,}:|\\Z)\",\n",
    "        'publications': r\"(Publications|Research Papers|Articles)(.*?)(?=\\n[A-Z][a-zA-Z ]{2,}:|\\Z)\",\n",
    "        'awards': r\"(Awards|Honors|Achievements)(.*?)(?=\\n[A-Z][a-zA-Z ]{2,}:|\\Z)\"\n",
    "    }\n",
    "    \n",
    "    for section, pattern in patterns.items():\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            sections[section] = match.group(2).strip()  # only the content, not the header\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d56e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_with_gemini_sections(sections):\n",
    "    \"\"\"\n",
    "    Send only relevant CV sections to Gemini LLM.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Extract structured CV information in the following JSON schema:\n",
    "    {{\n",
    "      \"education\": [{{\"degree\":\"\",\"field\":\"\",\"university\":\"\",\"country\":\"\",\"start\":null,\"end\":null,\"gpa\":null,\"scale\":null}}],\n",
    "      \"experience\": [{{\"title\":\"\",\"org\":\"\",\"start\":null,\"end\":null,\"duration_months\":null,\"domain\":\"\"}}],\n",
    "      \"publications\": [{{\"title\":\"\",\"venue\":\"\",\"year\":null,\"type\":\"\",\"authors\":[],\"author_position\":null,\"journal_if\":null,\"domain\":\"\"}}],\n",
    "      \"awards\": [{{\"title\":\"\",\"issuer\":\"\",\"year\":null,\"type\":\"\"}}]\n",
    "    }}\n",
    "\n",
    "    Only use the following CV content. Do not invent information:\n",
    "    Education: {sections['education']}\n",
    "    Experience: {sections['experience']}\n",
    "    Publications: {sections['publications']}\n",
    "    Awards: {sections['awards']}\n",
    "\n",
    "    Return only valid JSON. Do not add explanations or extra text.\n",
    "    \"\"\"\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0)\n",
    "    )\n",
    "\n",
    "    raw_text = resp.text.strip()\n",
    "    \n",
    "    # Safe JSON parsing\n",
    "    import re\n",
    "    try:\n",
    "        data = json.loads(raw_text)\n",
    "    except json.JSONDecodeError:\n",
    "        first_brace = raw_text.find(\"{\")\n",
    "        last_brace = raw_text.rfind(\"}\")\n",
    "        if first_brace != -1 and last_brace != -1:\n",
    "            raw_text = raw_text[first_brace:last_brace+1]\n",
    "        raw_text = re.sub(r',\\s*([}\\]])', r'\\1', raw_text)\n",
    "        try:\n",
    "            data = json.loads(raw_text)\n",
    "        except:\n",
    "            data = {\"education\": [], \"experience\": [], \"publications\": [], \"awards\": []}\n",
    "    \n",
    "    return data, raw_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1da8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(TEXT_DIR):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "    file_path = os.path.join(TEXT_DIR, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 1️⃣ Extract relevant sections using regex\n",
    "    sections = extract_sections(text)\n",
    "\n",
    "    # 2️⃣ Send sections to Gemini\n",
    "    structured_json, raw_resp = extract_json_with_gemini_sections(sections)\n",
    "\n",
    "    # 3️⃣ Save raw response\n",
    "    raw_file = os.path.join(RAW_DIR, filename.replace(\".txt\", \"_raw.txt\"))\n",
    "    with open(raw_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_resp)\n",
    "\n",
    "    # 4️⃣ Save structured JSON\n",
    "    json_file = os.path.join(JSON_DIR, filename.replace(\".txt\", \".json\"))\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(structured_json, f, indent=2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hr-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
