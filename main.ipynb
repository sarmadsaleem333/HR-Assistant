{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dccb0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning done. Log saved to docs/cleaning_log.txt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import io\n",
    "import pytesseract\n",
    "import fitz  \n",
    "import pytesseract\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "\n",
    "LOG_FILE = \"docs/cleaning_log.txt\"\n",
    "Path(CLEAN_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(\"docs\").mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Function to redact emails and phone numbers\n",
    "def redact_pii(text):\n",
    "    text = re.sub(r'\\b[\\w\\.-]+@[\\w\\.-]+\\.\\w+\\b', '[REDACTED_EMAIL]', text)\n",
    "    text = re.sub(r'\\b\\d{10,}\\b', '[REDACTED_PHONE]', text)\n",
    "    return text\n",
    "\n",
    "def is_pdf(file_path):\n",
    "    return file_path.lower().endswith(\".pdf\")\n",
    "\n",
    "def is_docx(file_path):\n",
    "    return file_path.lower().endswith(\".docx\")\n",
    "\n",
    "def detect_language(text):\n",
    "    try:\n",
    "        return detect(text)\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "\n",
    "\n",
    "def extract_text_pdf(file_path):\n",
    "    try:\n",
    "        doc = fitz.open(file_path)\n",
    "        full_text = \"\"\n",
    "        for page in doc:\n",
    "            # Try direct text first\n",
    "            page_text = page.get_text()\n",
    "            if page_text.strip():\n",
    "                full_text += page_text\n",
    "            else:\n",
    "                # Use OCR on page image\n",
    "                pix = page.get_pixmap(dpi=300)\n",
    "                img = Image.frombytes(\"RGB\", [pix.width, pix.height], pix.samples)\n",
    "                ocr_text = pytesseract.image_to_string(img, lang='eng')\n",
    "                full_text += ocr_text\n",
    "        return full_text if full_text.strip() else None\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading PDF {file_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def extract_text_docx(file_path):\n",
    "    try:\n",
    "        doc = Document(file_path)\n",
    "        return \"\\n\".join([p.text for p in doc.paragraphs])\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def process_files():\n",
    "    cleaning_log = []\n",
    "    seen_files = set()\n",
    "    for filename in os.listdir(RAW_DIR):\n",
    "        file_path = os.path.join(RAW_DIR, filename)\n",
    "\n",
    "        # Skip duplicates\n",
    "        if filename in seen_files:\n",
    "            cleaning_log.append((filename, \"duplicate\"))\n",
    "            continue\n",
    "        seen_files.add(filename)\n",
    "\n",
    "        # Unsupported format\n",
    "        if not (is_pdf(file_path) or is_docx(file_path)):\n",
    "            cleaning_log.append((filename, \"unsupported format\"))\n",
    "            continue\n",
    "\n",
    "        # Extract text\n",
    "        text = extract_text_pdf(file_path) if is_pdf(file_path) else extract_text_docx(file_path)\n",
    "        if not text or len(text.strip()) == 0:\n",
    "            cleaning_log.append((filename, \"corrupted or empty\"))\n",
    "            continue\n",
    "\n",
    "        # Detect language\n",
    "        lang = detect_language(text)\n",
    "        if lang != \"en\":\n",
    "            cleaning_log.append((filename, f\"non-English ({lang})\"))\n",
    "            continue\n",
    "\n",
    "        # Redact PII\n",
    "        text = redact_pii(text)\n",
    "\n",
    "        # Save cleaned file\n",
    "        shutil.copy(file_path, os.path.join(CLEAN_DIR, filename))\n",
    "        cleaning_log.append((filename, \"cleaned\"))\n",
    "\n",
    "    # Save log\n",
    "    with open(LOG_FILE, \"w\") as f:\n",
    "        for item in cleaning_log:\n",
    "            f.write(f\"{item[0]} : {item[1]}\\n\")\n",
    "    print(f\"Cleaning done. Log saved to {LOG_FILE}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_files()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fdf7e7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing done. Text files saved in data/text\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for filename in os.listdir(CLEAN_DIR):\n",
    "    file_path = os.path.join(CLEAN_DIR, filename)\n",
    "    text = \"\"\n",
    "\n",
    "    if filename.lower().endswith(\".pdf\"):\n",
    "        text = extract_text_pdf(file_path)  # <-- Use OCR-enabled function\n",
    "    elif filename.lower().endswith(\".docx\"):\n",
    "        text = extract_text_docx(file_path)\n",
    "    else:\n",
    "        continue  # skip unsupported files\n",
    "\n",
    "    if not text or len(text.strip()) == 0:\n",
    "        print(f\"Warning: No text extracted from {filename}\")\n",
    "        continue\n",
    "\n",
    "    # Save extracted text\n",
    "    text_file = os.path.join(TEXT_DIR, filename + \".txt\")\n",
    "    with open(text_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "\n",
    "print(f\"Parsing done. Text files saved in {TEXT_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ce9dc3e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON extraction done. Files saved in data/json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "TEXT_DIR = \"data/text\"\n",
    "JSON_DIR = \"data/json\"\n",
    "\n",
    "Path(JSON_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def extract_education(text):\n",
    "    education = []\n",
    "    # Simple regex examples\n",
    "    edu_matches = re.findall(r\"(BSc|MSc|PhD|Bachelor|Master|Doctor).+?([0-9]{4})\\s*-\\s*([0-9]{4})\", text)\n",
    "    for match in edu_matches:\n",
    "        degree, start, end = match\n",
    "        education.append({\n",
    "            \"degree\": degree,\n",
    "            \"field\": \"\",  # could be parsed further\n",
    "            \"university\": \"\",\n",
    "            \"country\": \"\",\n",
    "            \"start\": int(start),\n",
    "            \"end\": int(end),\n",
    "            \"gpa\": None,\n",
    "            \"scale\": None\n",
    "        })\n",
    "    return education\n",
    "\n",
    "def extract_experience(text):\n",
    "    experience = []\n",
    "    exp_matches = re.findall(r\"(\\w.+?),\\s*(\\w.+?),\\s*([0-9]{4}).*?([0-9]{4})\", text)\n",
    "    for match in exp_matches:\n",
    "        title, org, start, end = match\n",
    "        experience.append({\n",
    "            \"title\": title,\n",
    "            \"org\": org,\n",
    "            \"start\": int(start),\n",
    "            \"end\": int(end),\n",
    "            \"duration_months\": (int(end)-int(start))*12,\n",
    "            \"domain\": \"\"\n",
    "        })\n",
    "    return experience\n",
    "\n",
    "def extract_publications(text):\n",
    "    publications = []\n",
    "    pub_matches = re.findall(r'\"(.+?)\",\\s*(.+?),\\s*([0-9]{4})', text)\n",
    "    for match in pub_matches:\n",
    "        title, venue, year = match\n",
    "        publications.append({\n",
    "            \"title\": title,\n",
    "            \"venue\": venue,\n",
    "            \"year\": int(year),\n",
    "            \"type\": \"conference\",\n",
    "            \"authors\": [],\n",
    "            \"author_position\": None,\n",
    "            \"journal_if\": None,\n",
    "            \"domain\": \"\"\n",
    "        })\n",
    "    return publications\n",
    "\n",
    "def extract_awards(text):\n",
    "    awards = []\n",
    "    award_matches = re.findall(r'Best Paper Award|Awarded', text)\n",
    "    for match in award_matches:\n",
    "        awards.append({\n",
    "            \"title\": match,\n",
    "            \"issuer\": \"\",\n",
    "            \"year\": None,\n",
    "            \"type\": \"award\"\n",
    "        })\n",
    "    return awards\n",
    "\n",
    "for filename in os.listdir(TEXT_DIR):\n",
    "    file_path = os.path.join(TEXT_DIR, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    data = {\n",
    "        \"education\": extract_education(text),\n",
    "        \"experience\": extract_experience(text),\n",
    "        \"publications\": extract_publications(text),\n",
    "        \"awards\": extract_awards(text)\n",
    "    }\n",
    "\n",
    "    json_file = os.path.join(JSON_DIR, filename.replace(\".txt\", \".json\"))\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=2)\n",
    "\n",
    "print(f\"JSON extraction done. Files saved in {JSON_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4708a422",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dotenv in d:\\anaconda\\envs\\hr-assistant\\lib\\site-packages (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "! pip install python-dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fff66840",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gemini JSON extraction done. Raw responses in data/raw, cleaned JSON in data/json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "from pathlib import Path\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "GOOGLE_API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "# ---------------------- CONFIG ----------------------\n",
    "TEXT_DIR = \"data/text\"\n",
    "JSON_DIR = \"data/json\"\n",
    "RAW_DIR = \"data/raw\"  # Store raw LLM responses\n",
    "Path(JSON_DIR).mkdir(parents=True, exist_ok=True)\n",
    "Path(RAW_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize Gemini client\n",
    "client = genai.Client(api_key=GOOGLE_API_KEY)\n",
    "\n",
    "# ---------------------- FUNCTION ----------------------\n",
    "def extract_json_with_gemini(text):\n",
    "    \"\"\"\n",
    "    Send CV text to Gemini LLM and get structured JSON safely.\n",
    "    Returns (parsed_json, raw_response_text)\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Extract structured CV information in the following JSON schema:\n",
    "    {{\n",
    "      \"education\": [{{\"degree\":\"\",\"field\":\"\",\"university\":\"\",\"country\":\"\",\"start\":null,\"end\":null,\"gpa\":null,\"scale\":null}}],\n",
    "      \"experience\": [{{\"title\":\"\",\"org\":\"\",\"start\":null,\"end\":null,\"duration_months\":null,\"domain\":\"\"}}],\n",
    "      \"publications\": [{{\"title\":\"\",\"venue\":\"\",\"year\":null,\"type\":\"\",\"authors\":[],\"author_position\":null,\"journal_if\":null,\"domain\":\"\"}}],\n",
    "      \"awards\": [{{\"title\":\"\",\"issuer\":\"\",\"year\":null,\"type\":\"\"}}]\n",
    "    }}\n",
    "\n",
    "    CV Text:\n",
    "    {text}\n",
    "\n",
    "    Return **only valid JSON**. Do not add any explanation or extra text.\n",
    "    \"\"\"\n",
    "\n",
    "    # Call Gemini model\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0)\n",
    "    )\n",
    "\n",
    "    raw_text = resp.text.strip()\n",
    "\n",
    "    # Save raw response for auditing\n",
    "    raw_text_cleaned = raw_text.replace(\"\\ufeff\", \"\")  # remove BOM if any\n",
    "\n",
    "    # Attempt to parse JSON safely\n",
    "    parsed_json = None\n",
    "    if raw_text_cleaned:\n",
    "        try:\n",
    "            parsed_json = json.loads(raw_text_cleaned)\n",
    "        except json.JSONDecodeError:\n",
    "            # Fix common issues: trailing commas, extra text\n",
    "            first_brace = raw_text_cleaned.find(\"{\")\n",
    "            last_brace = raw_text_cleaned.rfind(\"}\")\n",
    "            if first_brace != -1 and last_brace != -1:\n",
    "                raw_text_cleaned = raw_text_cleaned[first_brace:last_brace+1]\n",
    "            # remove trailing commas\n",
    "            raw_text_cleaned = re.sub(r',\\s*([}\\]])', r'\\1', raw_text_cleaned)\n",
    "            try:\n",
    "                parsed_json = json.loads(raw_text_cleaned)\n",
    "            except json.JSONDecodeError:\n",
    "                # Fallback to empty schema\n",
    "                parsed_json = {\"education\": [], \"experience\": [], \"publications\": [], \"awards\": []}\n",
    "\n",
    "    else:\n",
    "        parsed_json = {\"education\": [], \"experience\": [], \"publications\": [], \"awards\": []}\n",
    "\n",
    "    return parsed_json, raw_text\n",
    "\n",
    "# ---------------------- MAIN LOOP ----------------------\n",
    "for filename in os.listdir(TEXT_DIR):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "    file_path = os.path.join(TEXT_DIR, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # Extract JSON using Gemini\n",
    "    structured_json, raw_resp = extract_json_with_gemini(text)\n",
    "\n",
    "    # Save raw response\n",
    "    raw_file = os.path.join(RAW_DIR, filename.replace(\".txt\", \"_raw.txt\"))\n",
    "    with open(raw_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_resp)\n",
    "\n",
    "    # Save cleaned JSON\n",
    "    json_file = os.path.join(JSON_DIR, filename.replace(\".txt\", \".json\"))\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(structured_json, f, indent=2)\n",
    "\n",
    "print(f\"Gemini JSON extraction done. Raw responses in {RAW_DIR}, cleaned JSON in {JSON_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcdc43be",
   "metadata": {},
   "source": [
    "With regex sections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "745078d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def extract_sections(text):\n",
    "    \"\"\"\n",
    "    Extract main CV sections using regex.\n",
    "    Returns a dictionary with keys: 'education', 'experience', 'publications', 'awards'\n",
    "    \"\"\"\n",
    "    sections = {'education': '', 'experience': '', 'publications': '', 'awards': ''}\n",
    "    \n",
    "    # Define regex patterns for section headers (case-insensitive)\n",
    "    patterns = {\n",
    "        'education': r\"(Education|Academic Background|Academic Qualifications)(.*?)(?=\\n[A-Z][a-zA-Z ]{2,}:|\\Z)\",\n",
    "        'experience': r\"(Experience|Work History|Employment)(.*?)(?=\\n[A-Z][a-zA-Z ]{2,}:|\\Z)\",\n",
    "        'publications': r\"(Publications|Research Papers|Articles)(.*?)(?=\\n[A-Z][a-zA-Z ]{2,}:|\\Z)\",\n",
    "        'awards': r\"(Awards|Honors|Achievements)(.*?)(?=\\n[A-Z][a-zA-Z ]{2,}:|\\Z)\"\n",
    "    }\n",
    "    \n",
    "    for section, pattern in patterns.items():\n",
    "        match = re.search(pattern, text, re.DOTALL | re.IGNORECASE)\n",
    "        if match:\n",
    "            sections[section] = match.group(2).strip()  # only the content, not the header\n",
    "\n",
    "    return sections\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d56e69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_json_with_gemini_sections(sections):\n",
    "    \"\"\"\n",
    "    Send only relevant CV sections to Gemini LLM.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Extract structured CV information in the following JSON schema:\n",
    "    {{\n",
    "      \"education\": [{{\"degree\":\"\",\"field\":\"\",\"university\":\"\",\"country\":\"\",\"start\":null,\"end\":null,\"gpa\":null,\"scale\":null}}],\n",
    "      \"experience\": [{{\"title\":\"\",\"org\":\"\",\"start\":null,\"end\":null,\"duration_months\":null,\"domain\":\"\"}}],\n",
    "      \"publications\": [{{\"title\":\"\",\"venue\":\"\",\"year\":null,\"type\":\"\",\"authors\":[],\"author_position\":null,\"journal_if\":null,\"domain\":\"\"}}],\n",
    "      \"awards\": [{{\"title\":\"\",\"issuer\":\"\",\"year\":null,\"type\":\"\"}}]\n",
    "    }}\n",
    "\n",
    "    Only use the following CV content. Do not invent information:\n",
    "    Education: {sections['education']}\n",
    "    Experience: {sections['experience']}\n",
    "    Publications: {sections['publications']}\n",
    "    Awards: {sections['awards']}\n",
    "\n",
    "    Return only valid JSON. Do not add explanations or extra text.\n",
    "    \"\"\"\n",
    "\n",
    "    resp = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash\",\n",
    "        contents=prompt,\n",
    "        config=types.GenerateContentConfig(temperature=0)\n",
    "    )\n",
    "\n",
    "    raw_text = resp.text.strip()\n",
    "    \n",
    "    # Safe JSON parsing\n",
    "    import re\n",
    "    try:\n",
    "        data = json.loads(raw_text)\n",
    "    except json.JSONDecodeError:\n",
    "        first_brace = raw_text.find(\"{\")\n",
    "        last_brace = raw_text.rfind(\"}\")\n",
    "        if first_brace != -1 and last_brace != -1:\n",
    "            raw_text = raw_text[first_brace:last_brace+1]\n",
    "        raw_text = re.sub(r',\\s*([}\\]])', r'\\1', raw_text)\n",
    "        try:\n",
    "            data = json.loads(raw_text)\n",
    "        except:\n",
    "            data = {\"education\": [], \"experience\": [], \"publications\": [], \"awards\": []}\n",
    "    \n",
    "    return data, raw_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4d1da8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(TEXT_DIR):\n",
    "    if not filename.endswith(\".txt\"):\n",
    "        continue\n",
    "    file_path = os.path.join(TEXT_DIR, filename)\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        text = f.read()\n",
    "\n",
    "    # 1️⃣ Extract relevant sections using regex\n",
    "    sections = extract_sections(text)\n",
    "\n",
    "    # 2️⃣ Send sections to Gemini\n",
    "    structured_json, raw_resp = extract_json_with_gemini_sections(sections)\n",
    "\n",
    "    # 3️⃣ Save raw response\n",
    "    raw_file = os.path.join(RAW_DIR, filename.replace(\".txt\", \"_raw.txt\"))\n",
    "    with open(raw_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(raw_resp)\n",
    "\n",
    "    # 4️⃣ Save structured JSON\n",
    "    json_file = os.path.join(JSON_DIR, filename.replace(\".txt\", \".json\"))\n",
    "    with open(json_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(structured_json, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e6208066",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page 1 OCR text length: 1446\n"
     ]
    }
   ],
   "source": [
    "import fitz\n",
    "from PIL import Image\n",
    "import pytesseract\n",
    "\n",
    "pytesseract.pytesseract.tesseract_cmd = r\"C:\\Program Files\\Tesseract-OCR\\tesseract.exe\"\n",
    "\n",
    "file_path = \"data/raw/sarmad_corrected.pdf\"\n",
    "doc = fitz.open(file_path)\n",
    "\n",
    "for i, page in enumerate(doc):\n",
    "    text = page.get_text()\n",
    "    if text.strip():\n",
    "        print(f\"Page {i+1}: extracted text directly.\")\n",
    "    else:\n",
    "        pix = page.get_pixmap(dpi=300)\n",
    "        img_bytes = pix.tobytes(\"png\")\n",
    "        img = Image.open(io.BytesIO(img_bytes))\n",
    "        img.save(f\"page_{i+1}.png\")  # Save image to check visually\n",
    "        ocr_text = pytesseract.image_to_string(img, lang=\"eng\")\n",
    "        print(f\"Page {i+1} OCR text length: {len(ocr_text)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149e1364",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hr-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
